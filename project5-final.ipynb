{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Technical Report - Data Engineering for company Gans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to database and create tables"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "We import all required packages and set a connection to the the database in the cloud:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For this project, we need to import the following packages:\n",
    "import requests\n",
    "import sqlalchemy\n",
    "import pandas as pd\n",
    "from pandas.io.json import json_normalize\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import numpy as np\n",
    "import time\n",
    "import datetime\n",
    "from datetime import date\n",
    "import mysql.connector\n",
    "\n",
    "\n",
    "\n",
    "############ name of the database we want to create\n",
    "dbname = \"project5\"\n",
    "\n",
    "############ list of cities we want to consider\n",
    "citylist = ['Frankfurt am Main', \"Munich\"]\n",
    "# citylist = ['Baden-Baden', \"Berlin\",  \"Bonn\", 'Bremen', 'Dresden', 'Dortmund', 'Düsseldorf', 'Essen', 'Frankfurt am Main', 'Hamburg', 'Hanover', 'Leipzig', \"Munich\", 'Münster',  'Nuremberg',  'Stuttgart']\n",
    "\n",
    "\n",
    "# At this point, we need to connect to the database in the cloud (we use the service AWS)\n",
    "############ AWS\n",
    "sethost = \"vdreiffm.cxmmedv6eu66.eu-central-1.rds.amazonaws.com\"\n",
    "setuser = \"admin\"\n",
    "setpassword = \"\"\n",
    "setport = 3306\n",
    "\n",
    "############ LOCAL\n",
    "# sethost = \"127.0.0.1\"\n",
    "# setuser = \"root\"\n",
    "# setpassword = \"\"\n",
    "# setport = 3306\n",
    "\n",
    "\n",
    "\n",
    "########### Access for SQLAlchemy\n",
    "host = sethost\n",
    "user = setuser\n",
    "password = setpassword\n",
    "port = setport\n",
    "con = f'mysql+pymysql://{user}:{password}@{host}:{port}/{dbname}'\n",
    "\n",
    "\n",
    "\n",
    "############ Access for SQLConnector\n",
    "import mysql.connector\n",
    "def connect():\n",
    "    return(mysql.connector.connect(\n",
    "    user=setuser,\n",
    "    password=setpassword,\n",
    "    host=sethost,\n",
    "    ))\n",
    "\n",
    "cnx = connect()\n",
    "cursor = cnx.cursor()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "At this point, we want to create the database \"dbname\" and its tables. To do so, we use SQL commands embeddet in python code via the mysql-connector. We also set primary and foreign keys for each table. These attributes and internal security routines in mysql ensure the consistency of the tables relations and consistency of new data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute(f\"CREATE DATABASE IF NOT EXISTS {dbname}\")\n",
    "\n",
    "cursor.execute(f\"USE {dbname}\")\n",
    "\n",
    "cursor.execute(\n",
    "    \"CREATE TABLE IF NOT EXISTS cities(\"\n",
    "    \"name VARCHAR(255),\"\n",
    "    \"country VARCHAR(255),\"\n",
    "    \"country_code VARCHAR(255),\"\n",
    "    \"wiki_data_id VARCHAR(255),\"\n",
    "    \"latitude NUMERIC,\"\n",
    "    \"longitude NUMERIC,\"\n",
    "    \"population INT,\"\n",
    "    \"timezone VARCHAR(255),\"\n",
    "    \"PRIMARY KEY(name)   )\"\n",
    ")\n",
    "\n",
    "cursor.execute(\n",
    "    \"CREATE TABLE IF NOT EXISTS weather(\"\n",
    "    \"id INT AUTO_INCREMENT,\"\n",
    "    \"city VARCHAR(255),\"\n",
    "    \"date_time date,\"\n",
    "    \"temperature INT,\"\n",
    "    \"rain VARCHAR(6),\"\n",
    "    \"clouds VARCHAR(255),\"\n",
    "    \"PRIMARY KEY(id),\"\n",
    "    \"FOREIGN KEY(city) REFERENCES cities(name)   )\"\n",
    ")\n",
    "\n",
    "cursor.execute(\n",
    "    \"CREATE TABLE IF NOT EXISTS airports(\"\n",
    "    \"icao VARCHAR(4),\"\n",
    "    \"airport VARCHAR(255),\"\n",
    "    \"city VARCHAR(255),\"\n",
    "    \"PRIMARY KEY(icao),\"\n",
    "    \"FOREIGN KEY(city) REFERENCES cities(name)   )\"\n",
    ")\n",
    "\n",
    "cursor.execute(\n",
    "    \"CREATE TABLE IF NOT EXISTS arrivals(\"\n",
    "    \"flightnumber VARCHAR(15),\"\n",
    "    \"status VARCHAR(255),\"\n",
    "    \"departure_airport_icao\tVARCHAR(255),\"\n",
    "    \"departure_airport_iata\tVARCHAR(255),\"\n",
    "    \"departure_airport_name\tVARCHAR(255),\"\n",
    "    \"arrival_scheduledTimeLocal\tDATETIME,\"\n",
    "    \"arrival_actualTimeLocal DATETIME,\"\n",
    "    \"arrival_scheduledTimeUtc DATETIME,\"\n",
    "    \"arrival_actualTimeUtc DATETIME,\"\n",
    "    \"arrival_terminal VARCHAR(255),\"\n",
    "    \"aircraft_model\tVARCHAR(255),\"\n",
    "    \"airline_name VARCHAR(255),\"\n",
    "    \"arrival_airport_icao VARCHAR(255),\"\n",
    "    \"PRIMARY KEY(flightnumber),\"\n",
    "    \"FOREIGN KEY(arrival_airport_icao) REFERENCES airports(icao)   )\"\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "We visualize the schema:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![fishy](project5_schema.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fill tables with collected data"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Now, we want to fill the tables with data. Due to the primary and foreign key relations, we have to start with the table \"cities\". Its primary key \"name\" is the foreign key of the tables \"weather\" and \"airports\". That means that one can only add new lines to the latter tables if a matching primary key (the city name) in \"cities\" exists. \n",
    "To fill each table, we will create a function that collects the favored data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# required packages\n",
    "import requests\n",
    "import sqlalchemy\n",
    "import pandas as pd\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "# the strategy is first to get the wikidataid and than collect the data from geoDB\n",
    "def demo(cities): # as input, we use a list of cities\n",
    "    cities_id = [] # initiate an empty id list\n",
    "    dfList = []\n",
    "    for city in cities:\n",
    "        #retrieve the wikidataId\n",
    "        time.sleep(1) # slows down the execution therby the server don't block our queries\n",
    "        url1 = f'https://en.wikipedia.org/wiki/{city}' #go to the wiki site of the city\n",
    "        citem = requests.get(url1, 'html.parser') # get the html\n",
    "        if BeautifulSoup(citem.content) != None:\n",
    "            soup = BeautifulSoup(citem.content)   # soup the content\n",
    "        if soup.find('li', {'id':'t-wikibase'}).find('a')['href'] != None:\n",
    "            wikidata_link = soup.find('li', {'id':'t-wikibase'}).find('a')['href'] #find the link that contains the wikidataid e.g. London https://www.wikidata.org/wiki/Q84\n",
    "        # wl.append(wikidata_link)\n",
    "        # \\d+ is a regular expression and means one digit or more, the wiki data id consist of a Q followed by severaldigits\n",
    "        #for group() in re see: https://www.tutorialspoint.com/What-is-the-groups-method-in-regular-expressions-in-Python\n",
    "        city_id = re.search('Q\\d+', wikidata_link).group()\n",
    "        cities_id.append(city_id)\n",
    "        #use the wikidataId to retrieve infrormation from geoDB\n",
    "        url2 = \"https://wft-geo-db.p.rapidapi.com/v1/geo/cities/{}\".format(city_id)\n",
    "        headers = {\n",
    "        \"X-RapidAPI-Key\": \"3cd15bf266msh2331a2a034ea490p1c96a2jsn828e9dce3a50\",\n",
    "        \"X-RapidAPI-Host\": \"wft-geo-db.p.rapidapi.com\"\n",
    "        }    \n",
    "        response = requests.request(\"GET\", url2, headers=headers)# gets a json-like string fromg geoDB containing the necessary informations\n",
    "        cit_dic = {}#make a dictionary to retrieve the information\n",
    "        cit_dic['name'] = response.json()['data']['name']\n",
    "        cit_dic['country'] = response.json()['data']['country']\n",
    "        cit_dic['country_code'] = response.json()['data']['countryCode']\n",
    "        cit_dic['wiki_data_id'] = response.json()['data']['wikiDataId']\n",
    "        cit_dic['latitude'] = round(response.json()['data']['latitude'], 4)\n",
    "        cit_dic['longitude'] = round(response.json()['data']['longitude'], 4)\n",
    "        cit_dic['population'] = response.json()['data']['population']\n",
    "        cit_dic['timezone'] = response.json()['data']['timezone']\n",
    "        \n",
    "        dfList.append(cit_dic) #put it in a list\n",
    "        df_demo = pd.DataFrame(dfList) # transform the list to df\n",
    "\n",
    "    return df_demo\n",
    "\n",
    "\n",
    "\n",
    "demodata = demo(citylist)  # stores the collected data in demodata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optionally, we save the data in a .csv file\n",
    "# demodata.to_csv('demodata.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# campare with the row above. We read the data from the genereted .csv\n",
    "demodata = pd.read_csv('demodata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we connect to the database and insert our data in the table \"cities\"\n",
    "demodata.to_sql(\"cities\", con=con, index=False, if_exists='append')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# required packages\n",
    "import json\n",
    "import sqlalchemy\n",
    "import requests\n",
    "import pandas as pd\n",
    "from pandas.io.json import json_normalize\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "def weather(cities): # as input, we use a list of cities\n",
    "    storage = pd.DataFrame() # here we store the data from each city\n",
    "    API_key = \"d3645498e615aef5ea56aba13e895b36\"   # the key for the API service\n",
    "    \n",
    "    for city in cities: # we iterate over each city\n",
    "        url = f\"http://api.openweathermap.org/data/2.5/forecast?q={city}&appid={API_key}&units=metric\"\n",
    "        response = requests.get(url)\n",
    "        wf = pd.DataFrame(response.json()[\"list\"])  # collecting the data\n",
    "        wf.drop([\"dt\", \"clouds\", \"wind\", \"visibility\", \"pop\", \"sys\"], axis=1, inplace=True)   #drop unnecessary columns\n",
    "        for i in range(wf.shape[0]):     # each element in the column weather is in a list. remove that list to bring it in JSON format\n",
    "            wf[\"weather\"][i] = wf[\"weather\"][i][0]\n",
    "        wf = pd.concat([wf, pd.json_normalize(wf.main), pd.json_normalize(wf.weather)], axis=1)   # flatten the columns main and weather\n",
    "        wf.drop([\"main\", \"weather\", \"feels_like\", \"pressure\", \"sea_level\", \"grnd_level\", \"humidity\", \"temp_kf\", \"id\", \"icon\", \"temp_min\", \"temp_max\"], axis=1, inplace=True)   #drop unnecessary columns\n",
    "        wf[\"city\"] = city  # generate a column with the city name\n",
    "        if 'rain' not in wf.columns:  #some cities have a column rain and some have not. No column means that there is no rain within the time frame\n",
    "            wf[\"rain\"] = pd.Series(dtype='object')\n",
    "        wf[\"rain\"] = wf.apply(lambda x: \"0\" if x[\"rain\"] is np.nan else \"1\", axis=1)  # 0 means no rain and 1 means rain\n",
    "        wf = wf.sort_index(axis=1) \n",
    "        wf.columns = [\"city\", \"clouds\", \"date_time\", \"rain\", \"temperature\"] # set new column names\n",
    "        wf = wf.sort_values(by=\"date_time\", ascending=True) # sort the data in respect of time\n",
    "        storage = pd.concat([storage, wf]).reset_index(drop=True) # we reset the index, so every row has its unique number\n",
    "    return storage\n",
    "\n",
    "\n",
    "\n",
    "weatherdata = weather(citylist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optionally, we save the data in a .csv file\n",
    "# weatherdata.to_csv('weatherdata.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# campare with the row above. We read the data from the genereted .csv\n",
    "weatherdata = pd.read_csv('weatherdata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we connect to the database and insert our data in the table \"weather\"\n",
    "weatherdata.to_sql(\"weather\", con=con, index=False, if_exists='append') # we use \"index=False\" because the primary key of weather has the attribute \"AUTO_INCREMENT\" and append the new data by assigning new ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table airports"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "In order to get the the arriving flights for a certain airport, we need its ICAO id. We will store the ids in the table \"airports\". This enables us to convert a airports name in the ICAO necessary for our querries.\n",
    "source: https://de.wikipedia.org/wiki/Liste_von_ICAO-Codes_in_Deutschland_mit_Flugplatzangaben"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "airportdata = [\n",
    "    ('EDSB', 'Karlsruhe/Baden-Baden', 'Baden-Baden'),\n",
    "    ('EDDB', 'Berlin Brandenburg', 'Berlin'),\n",
    "    (\"EDDT\", \"Berlin-Tegel\",\"Berlin\"),\n",
    "    (\"EDDK\", \"Köln/Bonn\", \"Bonn\"),\n",
    "    (\"EDDW\", \"Bremen\", \"Bremen\"),\n",
    "    (\"EDDC\", \"Dresden\", \"Dresden\"),\n",
    "    (\"EDLW\", \"Dortmund\", \"Dortmund\"),\n",
    "    (\"EDDL\", \"Düsseldorf\", \"Düsseldorf\"),\n",
    "    (\"EDLE\", \"Verkehrslandeplatz Essen/Mülheim\", \"Essen\"),\n",
    "    (\"EDDF\", \"Frankfurt am Main\", \"Frankfurt am Main\"),\n",
    "    (\"EDDH\", \"Hamburg\", \"Hamburg\"),\n",
    "    (\"EDDV\", \"Hannover-Langenhagen\", \"Hanover\"),\n",
    "    (\"EDDP\", \"Leipzig/Halle\", \"Leipzig\"),\n",
    "    (\"EDDM\", \"München\", \"Munich\"),\n",
    "    (\"EDDG\", \"Münster/Osnabrück\", \"Münster\"),\n",
    "    (\"EDDN\", \"Nürnberg\", \"Nuremberg\"),\n",
    "    (\"EDDS\", \"Stuttgart\", \"Stuttgart\")\n",
    "]\n",
    "\n",
    "aiports_df=pd.DataFrame(airportdata, columns = ['icao' , 'airport', 'city']) # creates a pandas data frame \n",
    "\n",
    "\n",
    "\n",
    "# we connect to the database and insert our data in the table \"airports\"\n",
    "aiports_df.to_sql(\"airports\", con=con, index=False, if_exists='append')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table arrivals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# required packages\n",
    "import sqlalchemy\n",
    "import requests\n",
    "import pandas as pd\n",
    "from pandas.io.json import json_normalize\n",
    "import json\n",
    "import numpy as np\n",
    "import datetime\n",
    "from datetime import date\n",
    "\n",
    "\n",
    "\n",
    "def arrivals(citylist): # as input, we use a list of cities  \n",
    "    \n",
    "    def city2iaco(citylist): # the API needs the ICAO code of the airport and not its name. So we write a function that converts the list of cityname to a list of the corresponding ICAOs\n",
    "        newcitylist = []\n",
    "        for city in citylist:\n",
    "            cnx = connect() # this function is defined at the beginning of this notebook. We need to reconnect to mysql after each query\n",
    "            cursor = cnx.cursor()\n",
    "            cursor.execute(f\"USE {dbname}\")   # again we use SQL\n",
    "            cursor.execute(f\"SELECT icao FROM airports WHERE city = '{city}'\")\n",
    "            cityname = cursor.fetchone()[0]  # the save the result of the query\n",
    "            newcitylist.append(cityname)\n",
    "        return(newcitylist)\n",
    "    airportlist = city2iaco(citylist) # now we proceed with the list of ICAOs\n",
    "    \n",
    "    storage = pd.DataFrame()   # here we store the data from each airport\n",
    "    # key = \"7a4bc5ce0bmshd49770f6283961fp1c45a6jsn405d55120cc4\"  # Marvin's key (not working anymore)\n",
    "    # key = \"515339d6fmsh81d78dce0cb28cap1cbb53jsnb31e102cdb8c\"   # Balus's key (not working anymore)\n",
    "    key = \"3cd15bf266msh2331a2a034ea490p1c96a2jsn828e9dce3a50\"    # Joachims's key\n",
    "    tomorrow = (date.today() + datetime.timedelta(days=1)).strftime(\"%Y-%m-%d\") # we want the arrivals of tomorrow and need its date\n",
    "    \n",
    "    for airport in airportlist:\n",
    "        url = f\"https://aerodatabox.p.rapidapi.com/flights/airports/icao/{airport}/{tomorrow}T00:00/{tomorrow}T12:00\"\n",
    "        querystring = {\"withLeg\":\"true\",\"direction\":\"Arrival\",\"withLocation\":\"true\"}\n",
    "        headers = {\n",
    "        \"X-RapidAPI-Host\": \"aerodatabox.p.rapidapi.com\",\n",
    "        \"X-RapidAPI-Key\": key}\n",
    "\n",
    "        response = requests.request(\"GET\", url, headers=headers, params=querystring)\n",
    "        arrivaldata = response.json()[\"arrivals\"]  # storing the data\n",
    "        arrivaldata = pd.DataFrame(pd.json_normalize(arrivaldata))\n",
    "        arrivaldata[\"arrival.airport.icao\"] = airport  # generate a column with the ICAO\n",
    "        arrivaldata.drop([\"codeshareStatus\", \"isCargo\", \"departure.quality\", \"arrival.quality\", \"aircraft.reg\", \"aircraft.modeS\", \"callSign\", \"departure.scheduledTimeLocal\", \"departure.actualTimeLocal\", \"departure.scheduledTimeUtc\", \"departure.actualTimeUtc\", \"departure.gate\", \"departure.terminal\", \"departure.checkInDesk\"], axis=1, inplace=True)  # remove unnecessary columns\n",
    "        arrivaldata.columns = [\"flightnumber\", \"status\", \"departure_airport_icao\", \"departure_airport_iata\", \"departure_airport_name\", \"arrival_scheduledTimeLocal\", \"arrival_actualTimeLocal\", \"arrival_scheduledTimeUtc\", \"arrival_actualTimeUtc\", \"arrival_terminal\", \"aircraft_model\", \"airline_name\", \"arrival_airport_icao\"]   # rename the columns.Especially \".\" are not allowed\n",
    "        # convert date-time columns in the right format:\n",
    "        arrivaldata['arrival_scheduledTimeLocal'] = pd.to_datetime(arrivaldata['arrival_scheduledTimeLocal']) \n",
    "        arrivaldata['arrival_actualTimeLocal'] = pd.to_datetime(arrivaldata['arrival_actualTimeLocal'])\n",
    "        arrivaldata['arrival_scheduledTimeUtc'] = pd.to_datetime(arrivaldata['arrival_scheduledTimeUtc'])\n",
    "        arrivaldata['arrival_actualTimeUtc'] = pd.to_datetime(arrivaldata['arrival_actualTimeUtc'])\n",
    "        storage = pd.concat([storage, arrivaldata]).reset_index(drop=True)\n",
    "    \n",
    "    return storage\n",
    "\n",
    "\n",
    "\n",
    "arrivaldata = arrivals([\"Frankfurt am Main\", \"Munich\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optionally, we save the data in a .csv file\n",
    "# arrivaldata.to_csv('arrivaldata.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# campare with the row above. We read the data from the genereted .csv\n",
    "arrivaldata = pd.read_csv('arrivaldata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we connect to the database and insert our data in the table \"arrivals\"\n",
    "arrivaldata.to_sql(\"arrivals\", con=con, index=False, if_exists='append')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the cloud (AWS): Setting a lambda function and CloudWatch"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "I created two lambda functions. One for the weather data and a manual use and one second test function with automation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lambda function I (collecting weather data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymysql\n",
    "import mysql.connector\n",
    "import sqlalchemy\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from pandas.io.json import json_normalize\n",
    "\n",
    "\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    \n",
    "    cnx = pymysql.connect(\n",
    "        user='admin',\n",
    "        password='',\n",
    "        host='vdreiffm.cxmmedv6eu66.eu-central-1.rds.amazonaws.com',\n",
    "        database='project5')\n",
    "    cursor = cnx.cursor()\n",
    "    ##################################################\n",
    "\n",
    "    dbname = \"project5\"\n",
    "    host=\"vdreiffm.cxmmedv6eu66.eu-central-1.rds.amazonaws.com\"\n",
    "    user=\"admin\"\n",
    "    password=\"\"\n",
    "    port=3306\n",
    "    con = f'mysql+pymysql://{user}:{password}@{host}:{port}/{dbname}'\n",
    "\n",
    "    \n",
    "    def weather(cities):\n",
    "        storage = pd.DataFrame()\n",
    "        API_key = \"d3645498e615aef5ea56aba13e895b36\"\n",
    "        \n",
    "        for city in cities:\n",
    "            url = f\"http://api.openweathermap.org/data/2.5/forecast?q={city}&appid={API_key}&units=metric\"\n",
    "            response = requests.get(url)\n",
    "            response\n",
    "            wf = pd.DataFrame(response.json()[\"list\"])\n",
    "            wf.drop([\"dt\", \"clouds\", \"wind\", \"visibility\", \"pop\", \"sys\"], axis=1, inplace=True)\n",
    "            for i in range(wf.shape[0]):     # each element of column weather is in a list. remove that list to bring it in JSON format\n",
    "                wf[\"weather\"][i] = wf[\"weather\"][i][0]\n",
    "            wf = pd.concat([wf, pd.json_normalize(wf.main), pd.json_normalize(wf.weather)], axis=1)   # flatten the columns main and weather\n",
    "            wf.drop([\"main\", \"weather\", \"feels_like\", \"pressure\", \"sea_level\", \"grnd_level\", \"humidity\", \"temp_kf\", \"id\", \"icon\", \"temp_min\", \"temp_max\"], axis=1, inplace=True)\n",
    "            wf[\"city\"] = city\n",
    "            if 'rain' not in wf.columns:  #some cities have a column rain and some not. If not, there is no rain at all\n",
    "                wf[\"rain\"] = pd.Series(dtype='object')\n",
    "            wf[\"rain\"] = wf.apply(lambda x: \"0\" if x[\"rain\"] is np.nan else \"1\", axis=1)  # 0 means no rain and 1 means rain\n",
    "            wf = wf.sort_index(axis=1)\n",
    "            wf.columns = [\"city\", \"clouds\", \"date_time\", \"rain\", \"temperature\"]\n",
    "            wf = wf.sort_values(by=\"date_time\", ascending=True)\n",
    "            storage = pd.concat([storage, wf]).reset_index(drop=True)\n",
    "        return(storage)\n",
    "    \n",
    "    \n",
    "    citylist = ['Baden-Baden', \"Berlin\"]\n",
    "    weatherdata = weather(citylist)\n",
    "    \n",
    "    weatherdata.to_sql(\"weather\", con=con, index=False, if_exists='append')\n",
    "    \n",
    "    # commit changes & close connection\n",
    "    cnx.commit()\n",
    "    cursor.close()\n",
    "    cnx.close()\n",
    "    \n",
    "    \n",
    "    #################################################\n",
    "    return {\n",
    "        'statusCode': 200,\n",
    "        'body': json.dumps('Hello from Lambda!')\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lambda function II (test function)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Due to the query limitations of the applied APIs, we use a test function for automation. For this, we create a dummy Database and table, we want to fill with dummy data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbname = \"auto_db\"\n",
    "tbname = \"auto_table\"\n",
    "conauto = f'mysql+pymysql://{user}:{password}@{host}:{port}/{dbname}'\n",
    "\n",
    "cursor.execute(f\"CREATE DATABASE IF NOT EXISTS {dbname}\")\n",
    "\n",
    "cursor.execute(f\"USE {dbname}\")\n",
    "\n",
    "cursor.execute(\n",
    "    f\"CREATE TABLE IF NOT EXISTS {tbname} (\"\n",
    "    \"time_data VARCHAR(255),\"\n",
    "    \"data VARCHAR(255) )\"\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The lambda function only connects to the database \"auto_db\" and inserts the actual date-time in the table \"auto_table\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pymysql\n",
    "import sqlalchemy\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from datetime import datetime\n",
    "\n",
    "  \n",
    "def lambda_handler(event, context):\n",
    "    \n",
    "    # connect to database\n",
    "    cnx = pymysql.connect(\n",
    "        user='admin',\n",
    "        password='',\n",
    "        host='vdreiffm.cxmmedv6eu66.eu-central-1.rds.amazonaws.com',\n",
    "        database='auto_db')\n",
    "          \n",
    "    cursor = cnx.cursor()\n",
    "    \n",
    "    \n",
    "    \n",
    "    ##################################################\n",
    "    \n",
    "    dbname = \"auto_db\"\n",
    "    host=\"vdreiffm.cxmmedv6eu66.eu-central-1.rds.amazonaws.com\"\n",
    "    user=\"admin\"\n",
    "    password=\"\"\n",
    "    port=3306\n",
    "    conauto = f'mysql+pymysql://{user}:{password}@{host}:{port}/{dbname}'\n",
    "  \n",
    "  \n",
    "    ##################################################\n",
    "    now = datetime.now().strftime(\"%Y-%m-%d %H:%M\")  # save the actual date and time\n",
    "\n",
    "    df = pd.DataFrame([{\"time_data\": now, \"data\": \"some data\"}])  # generate a data frame with a column with actual date-time and a dummy column with \"some data\"\n",
    "    df.to_sql(\"auto_table\", con=conauto, index=False, if_exists='append')   # the data frame is going to be appended in the database\n",
    "    ##################################################\n",
    "    \n",
    "  \n",
    "    # commit changes & close connection\n",
    "    cnx.commit()\n",
    "    cursor.close()\n",
    "    cnx.close()\n",
    "    \n",
    "    \n",
    "    return {\n",
    "        'statusCode': 200,\n",
    "        'body': json.dumps('Laeuft bei dir!')\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automation"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Now we want that the lambda function is called once an hour. We achieved this by creating an CloudWatch Event and connecting it with the lambda function. Typing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE auto_db;\n",
    "SELECT * FROM auto_table;"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "in the MySQL Workbench shows that everythin is working."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
